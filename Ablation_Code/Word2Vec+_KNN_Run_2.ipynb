{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**# Word 2 Vector + K Nearest Neighbour\n",
        "## Run 2"
      ],
      "metadata": {
        "id": "V6mzhcogJ_lh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08MBba7wAfT9",
        "outputId": "900bbdbd-dba7-4a69-82eb-ffd2308dd383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.96 µs\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "% time # to see wall time \n",
        "# Import required libraries\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Mount the Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the training data from Google Drive\n",
        "with open('/content/drive/MyDrive/Ablation/data/X_train.pkl', 'rb') as f:\n",
        "    datax_train = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ablation/data/Y_train.pkl', 'rb') as f:\n",
        "    datay_train = pickle.load(f)\n",
        "\n",
        "# Load the validation data from Google Drive\n",
        "with open('/content/drive/MyDrive/Ablation/data/X_valid.pkl', 'rb') as f:\n",
        "    datax_valid = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Ablation/data/Y_valid.pkl', 'rb') as f:\n",
        "    datay_valid = pickle.load(f)\n",
        "\n",
        "\n",
        "# Load the validatiotestn data from Google Drive\n",
        "with open('//content/drive/MyDrive/Ablation/data//X_test.pkl', 'rb') as f:\n",
        "    datax_test = pickle.load(f)\n",
        "    \n",
        "with open('//content/drive/MyDrive/Ablation/data//Y_test.pkl', 'rb') as f:\n",
        "    datay_test = pickle.load(f)\n",
        "\n",
        "\n",
        "#Code was used from Author Code\n",
        "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
        "    \"\"\"Create the matrices from the datasets.\n",
        "    This function pads each sequence to the same length: the length of the\n",
        "    longest sequence or maxlen. If maxlen is set, all sequences will be cut\n",
        "    to this maximum length. This function also swaps the axis.\n",
        "    \n",
        "    Args:\n",
        "        seqs (list): A list of sequences.\n",
        "        labels (list): A list of labels.\n",
        "        vocabsize (int): The size of the vocabulary.\n",
        "        maxlen (int): The maximum length of a sequence (optional).\n",
        "    \n",
        "    Returns:\n",
        "        tuple: A tuple containing the following arrays:\n",
        "            x (np.array): The input matrix of shape (n_samples, maxlen, vocabsize).\n",
        "            x_mask (np.array): The input mask matrix of shape (n_samples, maxlen).\n",
        "            y (np.array): The output matrix of shape (n_samples, maxlen).\n",
        "            lengths (list): A list of sequence lengths.\n",
        "            eventLengths (list): A list of event sequence lengths.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the lengths of all sequences\n",
        "    lengths = [len(s) for s in seqs]\n",
        "\n",
        "    # Concatenate all visits in each sequence to create event sequences\n",
        "    eventSeq = []\n",
        "    for seq in seqs:\n",
        "        t = []\n",
        "        for visit in seq:\n",
        "            t.extend(visit)\n",
        "        eventSeq.append(t)\n",
        "    eventLengths = [len(s) for s in eventSeq]\n",
        "\n",
        "    # If maxlen is set, truncate sequences longer than maxlen and discard\n",
        "    # sequences that are shorter than maxlen\n",
        "    if maxlen is not None:\n",
        "        new_seqs = []\n",
        "        new_lengths = []\n",
        "        new_labels = []\n",
        "        for l, s, la in zip(lengths, seqs, labels):\n",
        "            if l < maxlen:\n",
        "                new_seqs.append(s)\n",
        "                new_lengths.append(l)\n",
        "                new_labels.append(la)\n",
        "            else:\n",
        "                new_seqs.append(s[:maxlen])\n",
        "                new_lengths.append(maxlen)\n",
        "                new_labels.append(la[:maxlen])\n",
        "        lengths = new_lengths\n",
        "        seqs = new_seqs\n",
        "        labels = new_labels\n",
        "\n",
        "        # Return None if there are no sequences left after truncation\n",
        "        if len(lengths) < 1:\n",
        "            return None, None, None\n",
        "\n",
        "    n_samples = len(seqs)\n",
        "    maxlen = np.max(lengths)\n",
        "\n",
        "    # Initialize the input matrix, the input mask matrix, and the output matrix\n",
        "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
        "    x_mask = np.zeros((n_samples, maxlen)).astype('float64')\n",
        "    y = np.zeros((n_samples, maxlen)).astype('int64')\n",
        "\n",
        "    # Fill the input matrix with the one-hot-encoded events\n",
        "    for idx, s in enumerate(seqs):\n",
        "        x_mask[idx, :lengths[idx]] = 1\n",
        "        for j, sj in enumerate(s):\n",
        "            for tsj in sj:\n",
        "                x[idx, j, tsj-1] = 1\n",
        "\n",
        "    # Fill the output matrix with the labels\n",
        "    for idx, t in enumerate(labels):\n",
        "        y[idx,:lengths[idx]] = t\n",
        "\n",
        "    return x, x_mask, y, lengths, eventLengths\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code first mounts the Google Drive to access the data. It then loads the training, validation, and testing data from the pickle files.\n",
        "\n",
        "The prepare_data() function is then defined, which prepares the input data for the CNN model. This function converts the input sequences to a matrix with each row corresponding to a sequence of the same length. Each row contains the one-hot-encoded representation of the events in the sequence. The labels are also converted to a matrix, where each row corresponds to a sequence of labels.\n",
        "\n",
        "The code then prepares the training, testing, and validation data by calling the prepare_data() function with the required arguments.\n",
        "\n",
        "Next, the code imports necessary libraries from scikit-learn for classification tasks such as MultiLabelBinarizer, MultiOutputClassifier, LogisticRegression, SVC, GaussianNB, LinearDiscriminantAnalysis. The code also imports the required libraries from Keras for the CNN model such as Sequential, Dense, Flatten, Dropout, Conv1D, MaxPooling1D.\n",
        "\n",
        "After that, the code reshapes the training, validation, and testing input data to the required shape for the CNN model. Then it creates a CNN model using the Keras library, with two Conv1D layers, followed by a Dropout layer, MaxPooling1D layer, and two Dense layers with softmax activation.\n",
        "\n",
        "Finally, it extracts the features of the training data using the model_features object and prints them."
      ],
      "metadata": {
        "id": "P7r0qFqprsMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "WrQAlS7iH0Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWAIvdnGBAvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Feature set with word2Vec words that are saved with different lenghts\n",
        "#Parameter 1 - Dataset that needs to vectorized\n",
        "#Parameter 2 - The first number represents vocabulary size that needs to be choosen for samples\n",
        "#Parameter 3 - The secong number 50 represnets max lenght that needs to used to trim extra length sentecne \n",
        "#Code was used from Author Code\n",
        "# Prepare training data\n",
        "training = prepare_data(datax_train, datay_train, 100, 50)\n",
        "\n",
        "# Prepare testing data\n",
        "testing = prepare_data(datax_test, datay_test, 100, 50)\n",
        "\n",
        "# Prepare validation data\n",
        "validation = prepare_data(datax_valid, datay_valid, 100, 50)\n",
        "\n",
        "# Get training data features\n",
        "train_X = training[0]\n",
        "train_X = train_X.reshape(2800, 50*100)\n",
        "\n",
        "# Get training data labels\n",
        "train_Y = datay_train\n",
        "\n",
        "###################\n",
        "\n",
        "# Get validation data features\n",
        "valid_X = validation[0]\n",
        "valid_X = valid_X.reshape(100, 50*100)\n",
        "\n",
        "# Get validation data labels\n",
        "valid_Y = datay_valid\n",
        "\n",
        "####################\n",
        "\n",
        "# Get testing data features\n",
        "test_X = testing[0]\n",
        "test_X = test_X.reshape(100, 50*100)\n",
        "\n",
        "Get testing data labels\n",
        "test_Y = datay_test\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve,roc_auc_score,accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ],
      "metadata": {
        "id": "9Zz1lvKCBAyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The prepare data will return sequences and masks for input along with output\n",
        "# label for both training and testing\n",
        "\n",
        "YY=training[2] # extracting training labels for the dataset\n",
        "YY=np.array(YY) # array conversion\n"
      ],
      "metadata": {
        "id": "KKEcojiDBFFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the estimator and parameters for GridSearchCV\n",
        "estimator = MultiOutputClassifier(KNeighborsClassifier())\n",
        "parameters = {\n",
        "    'estimator__n_neighbors': [3, 5, 7],\n",
        "    'estimator__weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "# Define the GridSearchCV object with f1-score as the scoring metric\n",
        "clf = GridSearchCV(estimator, parameters, scoring='roc_auc')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "clf.fit(train_X, YY)\n",
        "\n",
        "# Print the best parameters and f1-score\n",
        "print('Best parameters:', clf.best_params_)\n",
        "print('Best roc-score:', clf.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV44_e41BXn6",
        "outputId": "7b898eb9-5f09-47f0-a956-a10041814166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'estimator__n_neighbors': 7, 'estimator__weights': 'uniform'}\n",
            "Best roc-score: 0.538679587092922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs hyperparameter tuning using GridSearchCV for a multi-output K-Nearest Neighbors classifier. Here's a breakdown of what's happening:\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV: import GridSearchCV from scikit-learn's model selection module.\n",
        "from sklearn.metrics import f1_score: import the f1_score metric from scikit-learn's metrics module (although it is not used in this code).\n",
        "from sklearn.neighbors import KNeighborsClassifier: import the KNeighborsClassifier from scikit-learn's neighbors module.\n",
        "from sklearn.multioutput import MultiOutputClassifier: import the MultiOutputClassifier from scikit-learn's multioutput module.\n",
        "estimator = MultiOutputClassifier(KNeighborsClassifier()): create a MultiOutputClassifier object with a KNeighborsClassifier estimator. This is the base model to be used in GridSearchCV.\n",
        "parameters = {'estimator__n_neighbors': [3, 5, 7], 'estimator__weights': ['uniform', 'distance']}: define the hyperparameters to be tuned. In this case, the number of neighbors (n_neighbors) and the type of weighting (weights) to be used in the KNeighborsClassifier.\n",
        "clf = GridSearchCV(estimator, parameters, scoring='roc_auc'): create a GridSearchCV object with the defined estimator, hyperparameters, and scoring metric (roc_auc). GridSearchCV performs an exhaustive search over the specified hyperparameters and returns the best combination of hyperparameters.\n",
        "clf.fit(train_X, YY): fit the GridSearchCV object to the training data (train_X and YY).\n",
        "print('Best parameters:', clf.best_params_): print the best hyperparameters found by GridSearchCV.\n",
        "print('Best roc-score:', clf.best_score_): print the best roc_auc score obtained by the best hyperparameters."
      ],
      "metadata": {
        "id": "tXSLYtgdxwue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KsKNppkAq_1w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwTHoASbw9kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Fit KNeighborsClassifier model with train_X and YY\n",
        "# Create a MultiOutputClassifier object for multiple outputs\n",
        "Model = MultiOutputClassifier(estimator=KNeighborsClassifier(n_neighbors=7, weights=\"uniform\")).fit(train_X, YY)\n",
        "\n",
        "# Use the trained model to predict labels for test_X\n",
        "y_pred = Model.predict(test_X)\n"
      ],
      "metadata": {
        "id": "eWDJtum4BcIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "YY=testing[2]# extracting testing  labels for the dataset\n",
        "test_YY=np.array(YY)# array conversion\n",
        "\n",
        "#list to store each sample performance\n",
        "PR_AUC=[]\n",
        "ACC=[]\n",
        "ROC_AUC=[]\n",
        "\n",
        "for i in range(test_X.shape[0]):\n",
        "  inter=test_X[i].reshape(1,-1)\n",
        "  y_score = Model.predict_proba(inter)\n",
        "  y_pred=Model.predict(inter)\n",
        "  proba=np.concatenate( y_score, axis=0)\n",
        "  probabilities=proba[:, 1]\n",
        "  #ROC_AUC, AU_PRC are define for only for input which has more than 1 classes in \n",
        "  #testing data for each sample , so we will calculate the score at that point only \n",
        "  #if not we will add zero to list\n",
        "  if test_YY[i].sum()>0:\n",
        "    auc_roc=roc_auc_score(test_YY[i],probabilities)\n",
        "    average_precision = average_precision_score(test_YY[i], probabilities)\n",
        "\n",
        "  else:\n",
        "    auc_roc=0 #setting zero if testing lables doesnot have more than 1 classes\n",
        "    average_precision=0 #setting zero if testing lables doesnot have more than 1 classes\n",
        "  acc=accuracy_score(test_YY[i],y_pred.reshape(-1,1))\n",
        "  PR_AUC.append(average_precision)\n",
        "  ACC.append(acc)\n",
        "  ROC_AUC.append(auc_roc)\n",
        "print('ACC score is',sum(ACC)/len(ACC))\n",
        "print('ROC_AUC score is',sum(ROC_AUC)/len(ROC_AUC))\n",
        "print('PR_AUC score is',sum(PR_AUC)/len(PR_AUC))\n",
        "%time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sW5qtehBA1a",
        "outputId": "9a254e24-d0b3-40fb-a792-164ad6c5ce8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC score is 0.7776000000000001\n",
            "ROC_AUC score is 0.5045758897999183\n",
            "PR_AUC score is 0.27099363150817973\n",
            "CPU times: user 10 µs, sys: 37 µs, total: 47 µs\n",
            "Wall time: 7.87 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates evaluation metrics such as accuracy, PR AUC, and ROC AUC for a given model on the testing data.\n",
        "\n",
        "The input_testing array contains the features for the testing data and the Model is the trained machine learning model. The loop iterates over each instance of the testing data and makes predictions using the trained model.\n",
        "\n",
        "The predict_proba method of the trained model is used to obtain the predicted probabilities of the target variable for each instance of the testing data. The probabilities are then used to calculate the PR AUC and ROC AUC scores using the average_precision_score and roc_auc_score functions from the sklearn.metrics module. If the true target variable contains at least one positive label, then the PR AUC and ROC AUC scores are calculated, otherwise, they are set to zero.\n",
        "\n",
        "The accuracy score is calculated using the accuracy_score function from the sklearn.metrics module. The true target variable is compared with the predicted target variable obtained from the predict method of the trained model.\n",
        "\n",
        "Finally, the calculated evaluation metrics are stored in the PR_AUC, ACC, and ROC_AUC lists, and the average scores are calculated and printed to the console."
      ],
      "metadata": {
        "id": "RTUBhU_yyFqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%whos # to see space"
      ],
      "metadata": {
        "id": "lX7UyzMtzfId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e8d0a2e-88fa-4ab8-b609-4002a849ca57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable                     Type                     Data/Info\n",
            "---------------------------------------------------------------\n",
            "ACC                          list                     n=100\n",
            "GaussianNB                   ABCMeta                  <class 'sklearn.naive_bayes.GaussianNB'>\n",
            "GridSearchCV                 ABCMeta                  <class 'sklearn.model_sel<...>on._search.GridSearchCV'>\n",
            "KNeighborsClassifier         ABCMeta                  <class 'sklearn.neighbors<...>on.KNeighborsClassifier'>\n",
            "LinearDiscriminantAnalysis   type                     <class 'sklearn.discrimin<...>earDiscriminantAnalysis'>\n",
            "LogisticRegression           type                     <class 'sklearn.linear_mo<...>stic.LogisticRegression'>\n",
            "Model                        MultiOutputClassifier    MultiOutputClassifier(est<...>lassifier(n_neighbors=7))\n",
            "MultiLabelBinarizer          type                     <class 'sklearn.preproces<...>bel.MultiLabelBinarizer'>\n",
            "MultiOutputClassifier        ABCMeta                  <class 'sklearn.multioutp<...>t.MultiOutputClassifier'>\n",
            "PR_AUC                       list                     n=100\n",
            "ROC_AUC                      list                     n=100\n",
            "SVC                          ABCMeta                  <class 'sklearn.svm._classes.SVC'>\n",
            "YY                           ndarray                  100x50: 5000 elems, type `int64`, 40000 bytes\n",
            "acc                          float64                  0.72\n",
            "accuracy_score               function                 <function accuracy_score at 0x7fdc00b2b910>\n",
            "auc_roc                      float64                  0.280701754385965\n",
            "average_precision            float64                  0.2064102564102564\n",
            "average_precision_score      function                 <function average_precisi<...>_score at 0x7fdc00b2a3b0>\n",
            "clf                          GridSearchCV             GridSearchCV(estimator=Mu<...>       scoring='roc_auc')\n",
            "datax_test                   list                     n=100\n",
            "datax_train                  list                     n=2800\n",
            "datax_valid                  list                     n=100\n",
            "datay_test                   list                     n=100\n",
            "datay_train                  list                     n=2800\n",
            "datay_valid                  list                     n=100\n",
            "drive                        module                   <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "estimator                    MultiOutputClassifier    MultiOutputClassifier(est<...>r=KNeighborsClassifier())\n",
            "f                            BufferedReader           <_io.BufferedReader name=<...>ive/MyDrive//Y_test.pkl'>\n",
            "f1_score                     function                 <function f1_score at 0x7fdc00b2bc70>\n",
            "i                            int                      99\n",
            "inter                        ndarray                  1x5000: 5000 elems, type `int64`, 40000 bytes\n",
            "np                           module                   <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
            "parameters                   dict                     n=2\n",
            "pd                           module                   <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
            "pickle                       module                   <module 'pickle' from '/u<...>ib/python3.10/pickle.py'>\n",
            "precision_recall_curve       function                 <function precision_recal<...>_curve at 0x7fdc00b2a710>\n",
            "prepare_data                 function                 <function prepare_data at 0x7fdc2ae4bf40>\n",
            "proba                        ndarray                  50x2: 100 elems, type `float64`, 800 bytes\n",
            "probabilities                ndarray                  50: 50 elems, type `float64`, 400 bytes\n",
            "roc_auc_score                function                 <function roc_auc_score at 0x7fdc00b2a560>\n",
            "test_X                       ndarray                  100x5000: 500000 elems, type `int64`, 4000000 bytes (3.814697265625 Mb)\n",
            "test_Y                       list                     n=100\n",
            "test_YY                      ndarray                  100x50: 5000 elems, type `int64`, 40000 bytes\n",
            "testing                      tuple                    n=5\n",
            "train_X                      ndarray                  2800x5000: 14000000 elems, type `int64`, 112000000 bytes (106.8115234375 Mb)\n",
            "train_Y                      list                     n=2800\n",
            "training                     tuple                    n=5\n",
            "valid_X                      ndarray                  100x5000: 500000 elems, type `int64`, 4000000 bytes (3.814697265625 Mb)\n",
            "valid_Y                      list                     n=100\n",
            "validation                   tuple                    n=5\n",
            "y_pred                       ndarray                  1x50: 50 elems, type `int64`, 400 bytes\n",
            "y_score                      list                     n=50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4RyGkd0H7-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}