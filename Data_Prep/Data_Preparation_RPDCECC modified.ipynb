{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from util import *\n",
        "\n",
        "#modify code from line 55 and line 59\n",
        "\n",
        "#from config import Config\n",
        "from patient_data_reader import PatientReader\n",
        "\n",
        "rare_word = 100\n",
        "stop_word = 1e4\n",
        "unknown = 1\n",
        "\n",
        "input_file = '/content/drive/MyDrive/S1_File.txt'\n",
        "vocab_file = '/content/vocab.txt'\n",
        "stop_file = '/content/stopwords.txt'\n",
        "vocab_pkl = '/content/vocab.pkl'\n",
        "\n",
        "\n",
        "def dump_vocab():\n",
        "    df = pd.read_csv(input_file, sep='\\t', header=0)\n",
        "    print(df[0:3])\n",
        "\n",
        "    # .to_frame(): indexed by the groups, with a custom name\n",
        "    # .reset_index(): set the groups to be columns again\n",
        "    hist = df.groupby('DX_GROUP_DESCRIPTION').size().to_frame('SIZE').reset_index()\n",
        "    print(hist[0:3])\n",
        "\n",
        "    # show some stats\n",
        "    hist_sort = hist.sort_values(by='SIZE', ascending=False)\n",
        "    print(hist_sort[0:3])\n",
        "    count = hist.groupby('SIZE').size().to_frame('COUNT').reset_index()\n",
        "    print(count)\n",
        "\n",
        "    # filter\n",
        "    hist = hist[hist['SIZE'] > rare_word]\n",
        "    print(hist)\n",
        "\n",
        "    # dump\n",
        "    vocab = hist.sort_values(by='SIZE').reset_index()['DX_GROUP_DESCRIPTION']\n",
        "    vocab.index += 2  # reserve 1 to unk\n",
        "    vocab.to_csv(vocab_file, sep='\\t', header=False, index=True)\n",
        "\n",
        "    # stop word\n",
        "    hist[hist['SIZE'] > stop_word].reset_index()['DX_GROUP_DESCRIPTION']\\\n",
        "        .to_csv(stop_file, sep='\\t', header=False, index=False)\n",
        "\n",
        "#####################\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "##############################\n",
        "\n",
        "def load_vocab():\n",
        "    word_to_index = {}\n",
        "    with tf.io.gfile.GFile(vocab_file, mode='r') as f:\n",
        "        line = f.readline()\n",
        "        while line != '':\n",
        "            tokens = line.strip().split('\\t')\n",
        "            word_to_index[tokens[0]] = tokens[0]\n",
        "            line = f.readline()\n",
        "    print('dict size: ' + str(len(word_to_index)))\n",
        "    save_pkl(vocab_pkl, {v: k for k, v in word_to_index.items()})\n",
        "    return word_to_index\n",
        "\n",
        "\n",
        "def convert_format(word_to_index, events):\n",
        "    # order by PID, DAY_ID\n",
        "    with open(input_file, mode='r') as f:\n",
        "        # header\n",
        "        header = f.readline().strip().split('\\t')\n",
        "        print(header)\n",
        "        pos = {}\n",
        "        for key, value in enumerate(header):\n",
        "            pos[value] = key\n",
        "        print(pos)\n",
        "\n",
        "        docs = []\n",
        "        doc = []\n",
        "        sent = []\n",
        "        labels = []\n",
        "        label = []\n",
        "\n",
        "        # init\n",
        "        line = f.readline()\n",
        "        tokens = line.strip().split('\\t')\n",
        "        pid = tokens[pos['PID']]\n",
        "        day_id = tokens[pos['DAY_ID']]\n",
        "        label.append(tag(events, pid, day_id))\n",
        "\n",
        "        while line != '':\n",
        "            tokens = line.strip().split('\\t')\n",
        "            c_pid = tokens[pos['PID']]\n",
        "            c_day_id = tokens[pos['DAY_ID']]\n",
        "\n",
        "            # closure\n",
        "            if c_pid != pid:\n",
        "                doc.append(sent)\n",
        "                docs.append(doc)\n",
        "                sent = []\n",
        "                doc = []\n",
        "                pid = c_pid\n",
        "                day_id = c_day_id\n",
        "                labels.append(label)\n",
        "                label = [tag(events, pid, day_id)]\n",
        "            else:\n",
        "                if c_day_id != day_id:\n",
        "                    doc.append(sent)\n",
        "                    sent = []\n",
        "                    day_id = c_day_id\n",
        "                    label.append(tag(events, pid, day_id))\n",
        "\n",
        "            word = tokens[pos['DX_GROUP_DESCRIPTION']]\n",
        "            try:\n",
        "                sent.append(word_to_index[word])\n",
        "            except KeyError:\n",
        "                sent.append(unknown)\n",
        "\n",
        "            line = f.readline()\n",
        "\n",
        "        # closure\n",
        "        doc.append(sent)\n",
        "        docs.append(doc)\n",
        "        labels.append(label)\n",
        "\n",
        "    return docs, labels\n",
        "\n",
        "\n",
        "def split_data(docs, labels):\n",
        "    # train, validate, test\n",
        "    # X, Y,\n",
        "    # TODO: YY\n",
        "    print(len(docs))\n",
        "    #print(docs)\n",
        "    print(len(labels))\n",
        "    print(docs)\n",
        "    print(labels)\n",
        "    #print(labels)\n",
        "\n",
        "    save_pkl('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_train.pkl', docs[:2800])\n",
        "    save_pkl('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_train.pkl', labels[:2800])\n",
        "    save_pkl('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_valid.pkl', docs[2800:2900])\n",
        "    save_pkl('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_valid.pkl', labels[2800:2900])\n",
        "    save_pkl('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_test.pkl', docs[2900:])\n",
        "    save_pkl('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_test.pkl', labels[2900:])\n",
        "\n",
        "\n",
        "def extract_events():\n",
        "    # extract event \"INPATIENT HOSPITAL\"\n",
        "    target_event = 'INPATIENT HOSPITAL'\n",
        "\n",
        "    df = pd.read_csv(input_file, sep='\\t', header=0)\n",
        "    events = df[df['SERVICE_LOCATION'] == target_event]\n",
        "\n",
        "    events = events.groupby(['PID', 'DAY_ID', 'SERVICE_LOCATION']).size().to_frame('COUNT').reset_index()\\\n",
        "        .sort_values(by=['PID', 'DAY_ID'], ascending=True)\\\n",
        "        .set_index('PID')\n",
        "\n",
        "    return events\n",
        "\n",
        "\n",
        "def tag(events, pid, day_id):\n",
        "    return 1 if tag_logic(events, pid, day_id) else 0\n",
        "\n",
        "\n",
        "def tag_logic(events, pid, day_id):\n",
        "    try:\n",
        "        patient = events.loc[int(pid)]\n",
        "\n",
        "        # test whether have events within 30 days\n",
        "        if isinstance(patient, pd.Series):\n",
        "            return (int(day_id) <= patient.DAY_ID) & (patient.DAY_ID < int(day_id) + 30)\n",
        "\n",
        "        return patient.loc[(int(day_id) <= patient.DAY_ID) & (patient.DAY_ID < int(day_id) + 30)].shape[0] > 0\n",
        "    except KeyError:\n",
        "        # the label is not in the [index]\n",
        "        return False\n",
        "\n",
        "\n",
        "def main():\n",
        "    # dump_vocab()\n",
        "    word_to_index = load_vocab()\n",
        "    events = extract_events()\n",
        "\n",
        "    docs, labels = convert_format(word_to_index, events)\n",
        "    split_data(docs, labels)\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-qZzRQRNl-5",
        "outputId": "6bde78d0-75a5-471f-f33a-8a8394e16cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict size: 30522\n",
            " [*] save /content/vocab.pkl\n",
            "['PID', 'DAY_ID', 'DX_GROUP_DESCRIPTION', 'SERVICE_LOCATION', 'OP_DATE']\n",
            "{'PID': 0, 'DAY_ID': 1, 'DX_GROUP_DESCRIPTION': 2, 'SERVICE_LOCATION': 3, 'OP_DATE': 4}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [*] save /content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_train.pkl\n",
            " [*] save /content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_train.pkl\n",
            " [*] save /content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_valid.pkl\n",
            " [*] save /content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_valid.pkl\n",
            " [*] save /content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_test.pkl\n",
            " [*] save /content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
        "    \"\"\"Create the matrices from the datasets.\n",
        "    This pad each sequence to the same lenght: the lenght of the\n",
        "    longuest sequence or maxlen.\n",
        "    if maxlen is set, we will cut all sequence to this maximum\n",
        "    lenght.\n",
        "    This swap the axis!\n",
        "    \"\"\"\n",
        "    # x: a list of sentences\n",
        "    lengths = [len(s) for s in seqs]\n",
        "\n",
        "    eventSeq = []\n",
        "\n",
        "    for seq in seqs:\n",
        "        t = []\n",
        "        for visit in seq:\n",
        "            t.extend(visit)\n",
        "        eventSeq.append(t)\n",
        "    eventLengths = [len(s) for s in eventSeq]\n",
        "\n",
        "\n",
        "    if maxlen is not None:\n",
        "        new_seqs = []\n",
        "        new_lengths = []\n",
        "        new_labels = []\n",
        "        for l, s, la in zip(lengths, seqs, labels):\n",
        "            if l < maxlen:\n",
        "                new_seqs.append(s)\n",
        "                new_lengths.append(l)\n",
        "                new_labels.append(la)\n",
        "            else:\n",
        "                new_seqs.append(s[:maxlen])\n",
        "                new_lengths.append(maxlen)\n",
        "                new_labels.append(la[:maxlen])\n",
        "        lengths = new_lengths\n",
        "        seqs = new_seqs\n",
        "        labels = new_labels\n",
        "\n",
        "        if len(lengths) < 1:\n",
        "            return None, None, None\n",
        "\n",
        "    n_samples = len(seqs)\n",
        "    maxlen = np.max(lengths)\n",
        "\n",
        "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
        "    x_mask = np.zeros((n_samples, maxlen)).astype('float64')\n",
        "    y = np.zeros((n_samples, maxlen)).astype('int64')\n",
        "    for idx, s in enumerate(seqs):\n",
        "        x_mask[idx, :lengths[idx]] = 1\n",
        "        for j, sj in enumerate(s):\n",
        "            for tsj in sj:\n",
        "                x[idx, j, tsj-1] = 1\n",
        "    for idx, t in enumerate(labels):\n",
        "        y[idx,:lengths[idx]] = t\n",
        "        # if lengths[idx] < maxlen:\n",
        "        #     y[idx,lengths[idx]:] = t[-1]\n",
        "\n",
        "    return x, x_mask, y, lengths, eventLengths\n"
      ],
      "metadata": {
        "id": "kdn0n0dj8LCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ub31tgC08ONr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_train.pkl', 'rb') as f:\n",
        "    datax_train = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_train.pkl', 'rb') as f:\n",
        "    datay_train = pickle.load(f)\n",
        "###################################################\n",
        "with open('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_valid.pkl', 'rb') as f:\n",
        "    datax_valid = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_valid.pkl', 'rb') as f:\n",
        "    datay_valid = pickle.load(f)\n",
        "#####################################################\n",
        "with open('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/X_test.pkl', 'rb') as f:\n",
        "    datax_test = pickle.load(f)\n",
        "    \n",
        "with open('/content/drive/MyDrive/Redrabbit RPDCECC/Data Preparation/Y_test.pkl', 'rb') as f:\n",
        "    datay_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "TRRx7a658RpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training=prepare_data(datax_train,datay_train,100, 50)\n",
        "testing=prepare_data(datax_test,datay_test,100,50)\n",
        "validation=prepare_data(datax_valid,datay_valid,100,50)"
      ],
      "metadata": {
        "id": "rhiX8Dv88T99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X=training[0]\n",
        "train_X=train_X.reshape(2800,50*100)\n",
        "train_Y=datay_train\n",
        "###################\n",
        "valid_X=validation[0]\n",
        "valid_X=valid_X.reshape(100,50*100)\n",
        "valid_Y=datay_valid\n",
        "####################\n",
        "test_X=testing[0]\n",
        "test_X=test_X.reshape(100,50*100)\n",
        "test_Y=datay_test"
      ],
      "metadata": {
        "id": "B729T5j28Wm2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}